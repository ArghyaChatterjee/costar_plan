#!/usr/bin/env python

from __future__ import print_function

from scipy.misc import imresize

import argparse
import cv2
import matplotlib.pyplot as plt
import numpy as np
import os
import sys
import h5py

from costar_models import *
from costar_models.planner import GetOrderedList, PrintTopQ
from costar_models.datasets.npz import NpzDataset
from costar_models.datasets.h5f import H5fDataset
from costar_models.datasets.npy_generator import NpzGeneratorDataset
from costar_models.datasets.h5f_generator import H5fGeneratorDataset
from costar_models.datasets.image import *

from costar_models.planner import *
from costar_models.multi import *
from costar_models.dvrk import MakeJigsawsImageClassifier

from costar_models.datasets.image import GetJpeg, JpegToNumpy

def convertJpegToImages(features):
    for i, f in enumerate(features):
        if str(f.dtype)[:2] == "|S":
            f = ConvertImageListToNumpy(np.squeeze(f))
            features[i] = f
    return features

def main(args):
    '''
    Tool for running model training without the rest of the simulation/planning/ROS
    code. This should be more or less independent and only rely on a couple
    external features.
    '''
    ConfigureGPU(args)

    np.random.seed(0)
    data_file_info = args['data_file'].split('.')
    data_type = data_file_info[-1]
    new_data_file = "small_" + data_file_info[0]
    try:
        os.mkdir(new_data_file)
    except OSError as e:
        pass
    root = ""
    for i, tok in enumerate(data_file_info[:-1]):
        if i < len(data_file_info)-1 and i > 0:
            root += '.'
        root += tok
    if data_type == "npz":
        dataset = NpzGeneratorDataset(root)
        data = dataset.load(success_only = args['success_only'])
    elif data_type == "h5f":
        dataset = H5fGeneratorDataset(root)
        data = dataset.load(success_only = args['success_only'])
    else:
        raise NotImplementedError('data type not implemented: %s'%data_type)

    if 'features' not in args or args['features'] is None:
        raise RuntimeError('Must provide features specification')
    features_arg = args['features']

    all_files = dataset.test + dataset.train
    idxs = range(len(all_files))
    np.random.shuffle(idxs)
    for fnum, filename in enumerate(all_files):
        newdata = {}
        ok = True
        data = dataset.loadFile(filename)
        for k, v in data.items():
            if v.shape[0] == 0:
                ok = False
                break

            if not k == "image":
                newdata[k] = v
            else:
                f = ConvertImageListToNumpy(np.squeeze(v))
                images = []
                for i in range(f.shape[0]):
                    frame = f[i]
                    #print(frame.shape)
                    dim = min(frame.shape[0], frame.shape[1]) - 80
                    crop = frame[80:(dim+80), 10:(dim+10), :]
                    image = imresize(crop, (96, 96))
                    image = image.astype(np.uint8)
                    #plt.figure()
                    #plt.imshow(image)
                    #plt.show()
                    images.append(GetJpeg(image))
                newdata[k] = np.array(images)
        #print("- total number of options =", data['labels_to_name'].shape)

        if not ok:
            print("Skipping because empty data for key %s" % k)
            continue

        ok = len(newdata['image']) == len(newdata['goal_idx'])
        if not ok:
            print("Skipping ", filename, "because data type shapes do"
                  " not match.")
            print("num goals =", len(data["goal_idx"]))
            print("num images =", len(data["image"]))
        else:
            toks = filename.split('.')
            filename = toks[0]
            success = toks[1]
            ext = toks[2]
            toks = filename.split('_')
            new_filename = ("example%06d" % (idxs[fnum])) + "." + success + "." + ext
            print(fnum, "Loaded", filename, "and saved to", new_filename)
            write(new_data_file, new_filename, newdata)

def write(directory, filename, data):
    filename = os.path.join(directory, filename)
    f = h5py.File(filename, 'w')
    for key, value in data.items():
        f.create_dataset(key, data=value)
    f.close()


info = """
This script will preprocess images collected from the real robot (with the 
CoSTAR setup), shrink them, crop slightly, and save as a new set of h5f files.
When specifying the data set, the name "small_" will be appended to the
beginning. Example usage:\n
\tpreprocess_images --data_file robot.h5f --features costar\n
This will write to:\n
\t./small_robot\n
which will contain a set of h5f files. Specifically, this code will enforce the
following changes:\n
\timage: resized, handled as jpeg
\tdepth: TODO
"""

if __name__ == '__main__':
    print(info)
    args = ParseModelArgs()
    if args['profile']:
        import cProfile
        cProfile.run('main(args)')
    else:
        main(args)
