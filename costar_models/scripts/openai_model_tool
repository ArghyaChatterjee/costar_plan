#!/usr/bin/env python

from costar_models import *
from costar_models.datasets.npz import NpzDataset

from costar_models.prednet import PredNet
from costar_models.data_utils import SequenceGenerator
from costar_models.openai_settings import *

import hickle as hkl
import os
from keras import backend as K
import numpy as np
np.random.seed(123)

from keras.models import Model
from keras.layers import Input, Dense, Flatten
from keras.layers import LSTM
from keras.layers import TimeDistributed
from keras.callbacks import LearningRateScheduler, ModelCheckpoint
from keras.optimizers import Adam






save_model = True  # if weights will be saved
weights_file = os.path.join(WEIGHTS_DIR, 'prednet_openai_weights.hdf5')  # where weights will be saved
json_file = os.path.join(WEIGHTS_DIR, 'prednet_openai_model.json')

'''
Tool for running model training without the rest of the simulation/planning/ROS
code. This should be more or less independent and only rely on a couple
external features.
'''

def main(args):
    if 'cpu' in args and args['cpu']:
        import tensorflow as tf

        with tf.device('/cpu:0'):
            config = tf.ConfigProto(
                device_count={'GPU': 0}
            )
            sess = tf.Session(config=config)
            K.set_session(sess)
            
    data = NpzDataset(args['data_file']).load()
    print "data keys are ", data.keys()
    print len(data['image'])
    source = "breakout"
    data['source'] = list([source] * len(data['image']))
    
    #[np.expand_dims(i, axis=0) for i in data['image']]

    #print data['image'][0].shape


    hkl.dump(data['image'], os.path.join(DATA_DIR, 'X_'  + '.hkl'))
    hkl.dump(data['source'], os.path.join(DATA_DIR, 'sources_'  + '.hkl'))

        # Data files
    train_file = os.path.join(DATA_DIR, 'X_.hkl')
    train_sources = os.path.join(DATA_DIR, 'sources_.hkl')
    val_file = os.path.join(DATA_DIR, 'X_.hkl')
    val_sources = os.path.join(DATA_DIR, 'sources_.hkl')

    # Training parameters
    nb_epoch = 150
    batch_size = 4
    samples_per_epoch = 500
    N_seq_val = 100  # number of sequences to use for validation

    # Model parameters
    n_channels, im_height, im_width = (1, 64, 64)
    print "image data format is " + K.image_data_format()
    input_shape = (n_channels, im_height, im_width) if K.image_data_format() == 'channels_first' else (im_height, im_width, n_channels)
    stack_sizes = (n_channels, 48, 96, 192)
    R_stack_sizes = stack_sizes
    A_filt_sizes = (3, 3, 3)
    Ahat_filt_sizes = (3, 3, 3, 3)
    R_filt_sizes = (3, 3, 3, 3)
    layer_loss_weights = np.array([1., 0., 0., 0.])  # weighting for each layer in final loss; "L_0" model:  [1, 0, 0, 0], "L_all": [1, 0.1, 0.1, 0.1]
    layer_loss_weights = np.expand_dims(layer_loss_weights, 1)
    nt = 10  # number of timesteps used for sequences in training
    time_loss_weights = 1./ (nt - 1) * np.ones((nt,1))  # equally weight all timesteps except the first
    time_loss_weights[0] = 0


    prednet = PredNet(stack_sizes, R_stack_sizes,
                      A_filt_sizes, Ahat_filt_sizes, R_filt_sizes,
                      output_mode='error', return_sequences=True)

    inputs = Input(shape=(nt,) + input_shape)
    errors = prednet(inputs)  # errors will be (batch_size, nt, nb_layers)
    errors_by_time = TimeDistributed(Dense(1, weights=[layer_loss_weights, np.zeros(1)], trainable=False), trainable=False)(errors)  # calculate weighted error by layer
    errors_by_time = Flatten()(errors_by_time)  # will be (batch_size, nt)
    final_errors = Dense(1, weights=[time_loss_weights, np.zeros(1)], trainable=False)(errors_by_time)  # weight errors by time
    model = Model(inputs=inputs, outputs=final_errors)
    model.compile(loss='mean_absolute_error', optimizer='adam')

    train_generator = SequenceGenerator(train_file, train_sources, nt, batch_size=batch_size, shuffle=True)
    val_generator = SequenceGenerator(val_file, val_sources, nt, batch_size=batch_size, N_seq=N_seq_val)

    lr_schedule = lambda epoch: 0.001 if epoch < 75 else 0.0001    # start with lr of 0.001 and then drop to 0.0001 after 75 epochs
    callbacks = [LearningRateScheduler(lr_schedule)]
    if save_model:
        if not os.path.exists(WEIGHTS_DIR): os.mkdir(WEIGHTS_DIR)
        callbacks.append(ModelCheckpoint(filepath=weights_file, monitor='val_loss', save_best_only=True))

    history = model.fit_generator(train_generator, samples_per_epoch / batch_size, nb_epoch, callbacks=callbacks,
                    validation_data=val_generator, validation_steps=N_seq_val / batch_size)

    if save_model:
        json_string = model.to_json()
        with open(json_file, "w") as f:
            f.write(json_string)




if __name__ == '__main__':
    args = ParseModelArgs()
    if args['profile']:
        import cProfile
        cProfile.run('main(args)')
    else:
        main(args)
